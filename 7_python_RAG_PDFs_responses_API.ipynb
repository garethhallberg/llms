{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 pandas tqdm openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import PyPDF2\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from utils.api_key_checker import APIKeyChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of APIKeyChecker\n",
    "checker = APIKeyChecker()\n",
    "\n",
    "# Call the check_keys method to print the API key status\n",
    "checker.check_keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = checker.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "dir_pdfs = 'mediterranean_noir_pdfs' # have those PDFs stored locally here\n",
    "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
    "\n",
    "print(f\"{len(pdf_files)} PDF files to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        return {\"file\": file_name, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
    "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
    "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
    "    \n",
    "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
    "            result = future.result()\n",
    "            if result[\"status\"] == \"success\":\n",
    "                stats[\"successful_uploads\"] += 1\n",
    "            else:\n",
    "                stats[\"failed_uploads\"] += 1\n",
    "                stats[\"errors\"].append(result)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def create_vector_store(store_name: str) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create(name=store_name)\n",
    "        details = {\n",
    "            \"id\": vector_store.id,\n",
    "            \"name\": vector_store.name,\n",
    "            \"created_at\": vector_store.created_at,\n",
    "            \"file_count\": vector_store.file_counts.completed\n",
    "        }\n",
    "        print(\"Vector store created:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = \"mediterranean_noir_store\"\n",
    "vector_store_details = create_vector_store(store_name)\n",
    "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message):\n",
    "    query = message\n",
    "    response = client.responses.create(\n",
    "        input= query,\n",
    "        instructions=\"Answer the question with as much detail as you can.\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_details['id']],\n",
    "        }]\n",
    "    )\n",
    "    print(response.output)\n",
    "\n",
    "    if len(response.output) == 1:\n",
    "        print(response)\n",
    "        return response.output[0].content[0].text\n",
    "    \n",
    "\n",
    "    # Extract annotations from the response\n",
    "    annotations = response.output[1].content[0].annotations\n",
    "    \n",
    "    # # Get top-k retrieved filenames\n",
    "    retrieved_files = set([result.filename for result in annotations])\n",
    "\n",
    "    answer = f'Files used: {retrieved_files}' + '\\n' + 'Response:' + response.output[1].content[0].text\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=chat, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
