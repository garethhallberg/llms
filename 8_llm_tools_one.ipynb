{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "from search_tool import web_search, extract_content\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_query(query: str, history: str) -> str:\n",
    "    client = AsyncClient()\n",
    "\n",
    "    # Define our search tool\n",
    "    search_tool = {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'web_search',\n",
    "            'description': 'Search the web for current information on a topic',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'required': ['query'],\n",
    "                'properties': {\n",
    "                    'query': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The search query to look up'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    system_message = \"Answer questions like you are a Yorkshire man\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    # First, let Ollama decide if it needs to search\n",
    "    response = await client.chat(\n",
    "        'llama3.2',\n",
    "        messages = messages,\n",
    "        tools=[search_tool]\n",
    "    )\n",
    "\n",
    "    # Initialize available functions\n",
    "    available_functions = {\n",
    "        'web_search': web_search\n",
    "    }\n",
    "\n",
    "    # Check if Ollama wants to use the search tool\n",
    "    if response.message.tool_calls:\n",
    "        print(\"Searching the web...\")\n",
    "\n",
    "        for tool in response.message.tool_calls:\n",
    "            if function_to_call := available_functions.get(tool.function.name):\n",
    "                # Call the search function\n",
    "                search_results = function_to_call(**tool.function.arguments)\n",
    "\n",
    "                if \"error\" in search_results:\n",
    "                    if search_results[\"error\"] == \"authentication_failed\":\n",
    "                        return \"Authentication failed. Please check your API key.\"\n",
    "                    return f\"Search error: {search_results['error']}\"\n",
    "\n",
    "                # Extract relevant content\n",
    "                content = extract_content(search_results)\n",
    "\n",
    "                if not content:\n",
    "                    return \"No relevant information found.\"\n",
    "\n",
    "                # Add the search results to the conversation\n",
    "                messages = [\n",
    "                    {'role': 'user', 'content': query},\n",
    "                    response.message,\n",
    "                    {\n",
    "                        'role': 'tool',\n",
    "                        'name': tool.function.name,\n",
    "                        'content': content\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                # Get final response from Ollama with the search results\n",
    "                final_response = await client.chat(\n",
    "                    'llama3.2',\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                return final_response.message.content\n",
    "\n",
    "    # If no tool calls, return the direct response\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n",
      "Searching the web...\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "# question = \"How can my knee pain be resolved? \"\n",
    "# print(\"\\nProcessing your question...\")\n",
    "# answer = await process_query(question)\n",
    "# print(\"\\nAnswer:\")\n",
    "# print(answer)\n",
    "\n",
    "# gr.Interface(fn=process_query, inputs=\"textbox\", outputs=\"textbox\").launch()\n",
    "\n",
    "#use the gradio chat interface\n",
    "gr.ChatInterface(fn=process_query, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
